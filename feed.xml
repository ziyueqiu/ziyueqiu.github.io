<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" xml:lang="en"><generator uri="https://jekyllrb.com/" version="4.4.1">Jekyll</generator><link href="https://ziyueqiu.github.io/feed.xml" rel="self" type="application/atom+xml"/><link href="https://ziyueqiu.github.io/" rel="alternate" type="text/html" hreflang="en"/><updated>2025-11-29T20:33:46-05:00</updated><id>https://ziyueqiu.github.io/feed.xml</id><title type="html">blank</title><subtitle>A simple, whitespace theme for academics. Based on [*folio](https://github.com/bogoli/-folio) design. </subtitle><entry><title type="html">To Move or Not to Move - The Economics of Cloud Computing (HotCloud 11) 论文阅读</title><link href="https://ziyueqiu.github.io/blog/2023/CloudEconomics-(HotCloud-11)/" rel="alternate" type="text/html" title="To Move or Not to Move - The Economics of Cloud Computing (HotCloud 11) 论文阅读"/><published>2023-11-16T00:00:00-05:00</published><updated>2023-11-16T00:00:00-05:00</updated><id>https://ziyueqiu.github.io/blog/2023/CloudEconomics%20(HotCloud%2011)</id><content type="html" xml:base="https://ziyueqiu.github.io/blog/2023/CloudEconomics-(HotCloud-11)/"><![CDATA[<p><a href="https://www.usenix.org/legacy/event/hotcloud11/tech/final_files/Tak.pdf">To Move or Not to Move: The Economics of Cloud Computing</a></p> <p>本文在 2011 年尝试做出了对往后十年的预测，有一些现在看来并不准确，但有一些地方作为早期的观察是很敏锐的。我抱着敬畏之心记录至今仍有借鉴意义的点。作为一个既不太懂 cloud，也不太懂 economy 的人，欢迎评论区纠正！</p> <h2 id="introduction--background">Introduction &amp; Background</h2> <p>Cloud-based hosting promises several advantages:</p> <ul> <li>Ease-of-management: since the cloud provider assumes management-related responsibilities, the customer is relieved of this burden and can focus on its core expertise.</li> <li>Cap-ex savings: it eliminates the need for purchasing infrastructure; this may translate into lowering the business entry barrier.</li> <li>Op-ex reduction: elimination of the need to pay for salaries, utility electricity bills, real-estate rents/mortgages, etc. One oft-touted aspect of Op-ex savings concerns the ability of customer’s Op-ex to closely match its evolving resource needs (via usage-based charging) as opposed to depending on its worst-case needs.</li> </ul> <p>An interesting comment: using the cloud need not preclude a continued use of in-house infrastructure. The most cost-effective approach for an organization might, in fact, involve a combination of cloud and in-house resources rather than choosing one over the other.</p> <p>Two types of partitioning:</p> <ul> <li>Vertical partitioning splits an application into two subsets (not necessarily mutually exclusive) of components - one is hosted in-house and the other migrated to the cloud</li> <li>Horizontal partitioning replicates some components of the application (or the entire application) on the cloud along with suitable workload distribution mechanisms. Such partitioning is already being used as a way to handle unexpected traffic bursts by some businesses.</li> </ul> <h2 id="key-results">Key Results</h2> <h3 id="workload-intensity">Workload Intensity</h3> <p>In-house provisioning is cost-effective for medium to large workloads, whereas cloud-based options suit small workloads. For small workloads, the servers procured for in-house provisioning end up having significantly more capacity than needed (and they remain under-utilized) since they are the lowest granularity servers available in market today. On the other hand, cloud can offer instances matching the small workload needs (due to the statistical multiplexing and virtualization it employs).</p> <h3 id="workload-growth">Workload Growth</h3> <p>This paper assumes “hardware capacity growing according to Moore’s Law, unless the workload growth matches or exceeds this rate, the number of servers required in-house will actually shrink each year. However, things evolve differently with cloud-based options. The computing power as well as price of a cloud instance are intentionally engineered to be at a certain level (via virtualization and statistical multiplexing) even though cloud providers may upgrade their hardware regularly (just as in-house). E.g., since the start of EC2 in 2006, the computing power/memory per instance has remained unchanged while there has been only one occasion of instance price reduction. In other words, while in-house hosting enjoys improvement in performance/$ with time, trends over the last 5 years suggest that <strong>the performance/$ offered by the cloud has remained unchanged</strong>. Even if we assume the performance/$ offered by the cloud improves with time (say, an instance of given capacity becomes cheaper over time), cloud-based provisioning still remains expensive in the long run since data capacity and transfer costs contribute to the costs more significantly than in-house.”</p> <p>We don’t believe this is valid: at the beginning, AWS provided c4.xlarge and its prices may actually haven’t changed for the last decade. But after some period of time, they started to provide upgraded VM, c5.xlarge, which is even cheaper than c4.xlarge with better performance. Similar to what’s happening in in-house case.</p> <h3 id="data-transfer">Data Transfer</h3> <p>Data transfer is a significant contributor to the costs of cloud-based hosting. This suggests that vertical partitioning choices may not be appealing for applications that exchange data with the external world.</p> <h3 id="workload-variance-and-cloud-elasticity">Workload Variance and Cloud Elasticity</h3> <p>Horizontal partitioning can be effectively used to eliminate the cost increase from provisioning for the peak.</p> <h2 id="评论">评论</h2> <p>简单来说，本文的一个重要假设 - cloud 的性价比会越来越差 - 严重影响了大部分的计算结果，而我们不太认同。但其他几个比较独立的观察 - data transfer 很贵、horizontal partitioning 天然地适配 burst load to cloud 的情况 - 应该是蛮先锋的断言。</p>]]></content><author><name></name></author><category term="paperreading"/><category term="cloud"/><summary type="html"><![CDATA[To Move or Not to Move: The Economics of Cloud Computing]]></summary></entry><entry><title type="html">RocksDBWorkload (FAST 20) 论文阅读</title><link href="https://ziyueqiu.github.io/blog/2023/RocksDBWorkload-(FAST-20)/" rel="alternate" type="text/html" title="RocksDBWorkload (FAST 20) 论文阅读"/><published>2023-06-20T00:00:00-04:00</published><updated>2023-06-20T00:00:00-04:00</updated><id>https://ziyueqiu.github.io/blog/2023/RocksDBWorkload%20(FAST%2020)</id><content type="html" xml:base="https://ziyueqiu.github.io/blog/2023/RocksDBWorkload-(FAST-20)/"><![CDATA[<p><a href="https://www.usenix.org/system/files/fast20-cao_zhichao.pdf">Characterizing, Modeling, and Benchmarking RocksDB Key-Value Workloads at Facebook</a></p> <p>我比较关心 benchmarking，而且本文感觉有追随 Sigmetrics 12 的工作，我挑不同的地方记录。而且已经有一些 blog 写这篇文章，我就尽量精（tou）简（lan）了。</p> <h2 id="introduction">Introduction</h2> <ul> <li>the accesses in UDB explicitly exhibit a diurnal pattern – How to model diurnal patterns?</li> <li>The whole key-space is partitioned into small key-ranges, and we model the hotness of these small key-ranges.</li> <li>YCSB causes at least 500% more read-bytes and delivers only 17% of the cache hits in RocksDB compared with real-world workloads. New benchmark has only 43% more read-bytes and achieve about 77% of the cache hits in RocksDB, and thus are much closer to real-world workloads.</li> </ul> <h2 id="modeling-and-benchmarking">Modeling and Benchmarking</h2> <p>We first calculate the Pearson correlation coefficients between any two selected variables to ensure that these variables have very low correlations. In this way, each variable can be modeled separately. Then, we fit the collected workloads to different statistical models to find out which one has the lowest fitting error, which is more accurate than always fitting different workloads to the same model (like Zipfian). The proposed benchmark can then generate KV queries based on these probability models.</p> <p>QPS (Queries Per Second) of some CFs in UDB (social network SQL) have strong diurnal patterns</p> <h3 id="key-space-and-temporal-patterns">Key-Space and Temporal Patterns</h3> <p>We sort all the existing keys <strong>in the same order as they are stored in RocksDB</strong> and plot out the access count of each KV-pair, which is called the heat-map of the whole key-space.</p> <p>They mentioned: The hotness of these key-ranges is closely related to cache efficiency and generated storage I/Os. The better a key-space locality is, the higher the RocksDB block cache hit ratio will be.</p> <h3 id="key-range-based-modeling">Key-Range Based Modeling</h3> <p>We first fit the distributions of key sizes, value sizes, and QPS to different mathematical models (e.g., Power, Exponential, Polynomial, Webull, Pareto, and Sine) and select the model that has the minimal fit standard error (FSE). This is also called the root mean squared error.</p> <p>QPS can be better fit to <strong>Cosine or Sine</strong> in a 24-hour period with very small amplitude</p> <p>Details:</p> <p>Based on the KV-pair access counts and their sequence in the whole key-space, the average accesses per KV-pair of each key-range is calculated and fit to the distribution model (e.g., power distribution). This way, when one query is generated, we can calculate the probability of each key-range responding to this query. Inside each key range, we let the KV-pair access count distribution follow the distribution of the whole key-space. This ensures that the distribution of the overall KV-pair access counts satisfies that of a real-world workload. Also, we make sure that <strong>hot KV-pairs are allocated closely together</strong>. Hot and cold key-ranges can be randomly assigned to the whole key-space, since the locations of keyranges have low influence on the workload locality.</p> <p>我的理解是：</p> <ul> <li>首先对于多个 key-range，各自有不同的热度，他们之间的位置的相对关系不重要，生成的时候先根据概率选到某个 key-range；</li> <li>对于每个 key range，不区分他们的 distribution，都使用整体的 distribution（e.g. power distribution）来区分冷热块；</li> <li>需要注意 hot KV-pairs 还需要物理上接近</li> </ul> <p>目前为止我的疑问有：</p> <ul> <li>怎么切分 key-range？太细或者太粗粒度效果都不好</li> <li>怎么让 hot KV-pairs 物理上接近？</li> <li>如何处理 request type generation？Put, Get, Delete, etc；我认为不同类型的 request，distribution 也不同</li> </ul>]]></content><author><name></name></author><category term="paperreading"/><category term="modeling"/><summary type="html"><![CDATA[Characterizing, Modeling, and Benchmarking RocksDB Key-Value Workloads at Facebook]]></summary></entry><entry><title type="html">MemcachedWorkload (Sigmetrics 12) 论文阅读</title><link href="https://ziyueqiu.github.io/blog/2023/MemcachedWorkload-(Sigmetrics-12)/" rel="alternate" type="text/html" title="MemcachedWorkload (Sigmetrics 12) 论文阅读"/><published>2023-06-19T00:00:00-04:00</published><updated>2023-06-19T00:00:00-04:00</updated><id>https://ziyueqiu.github.io/blog/2023/MemcachedWorkload%20(Sigmetrics%2012)</id><content type="html" xml:base="https://ziyueqiu.github.io/blog/2023/MemcachedWorkload-(Sigmetrics-12)/"><![CDATA[<p><a href="https://ranger.uta.edu/~sjiang/pubs/papers/atikoglu12-memcached.pdf">Workload Analysis of a Large-Scale Key-Value Store</a></p> <p>本文我比较关心 workload generation，其他部分我会挑值得记录的写一写。</p> <h2 id="overview">Overview</h2> <ul> <li>analyze five workloads from Facebook’s Memcached deployment</li> <li>over 284 billion requests over a period of 58 sample days</li> <li>analyze request composition, size and rate, cache efficacy, and temporal patterns</li> <li>propose a Model to generate synthetic workloads</li> </ul> <h2 id="statistical-modeling">Statistical Modeling</h2> <p>Use ETC trace, obviously not include all the nuances of the trace such as its bursty nature or the inclusion of one-off events.</p> <ul> <li>Emulate three properties: key size, value size, and inter-arrival rate[‘the time gap in microseconds between consecutive received requests’]</li> <li>Use Pearson correlation coefficient, find these three are independent</li> </ul> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/fig/FacebookWorkload-fig1-480.webp"/> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/fig/FacebookWorkload-fig1-800.webp"/> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/fig/FacebookWorkload-fig1-1400.webp"/> <img src="/assets/img/fig/FacebookWorkload-fig1.png" class="img-fluid rounded z-depth-1" width="auto" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <ul> <li>Fit distributions (such as Weibull, Gamma, Extreme Value, Normal, etc.) to each data set, and choose the one that minimizes the Kolmogorov-Smirnov distance</li> <li>Diurnal: divide raw data into 24 hourly bins and model each</li> </ul> <h2 id="评论">评论</h2> <p>本文虽然号称是对 memcached 这个 caching engine 做的 workload 分析，实际上完全不能用来生成 caching workloads，因为甚至没有 differentiate different objects，那就是 uniform distribution，那就应该做 tiering 或者直接放一些数据再也不动了。</p>]]></content><author><name></name></author><category term="paperreading"/><category term="modeling"/><category term="cache"/><summary type="html"><![CDATA[Workload Analysis of a Large-Scale Key-Value Store]]></summary></entry><entry><title type="html">JEDI (IMC 22) 论文阅读</title><link href="https://ziyueqiu.github.io/blog/2023/JEDI-(IMC-22)/" rel="alternate" type="text/html" title="JEDI (IMC 22) 论文阅读"/><published>2023-06-06T00:00:00-04:00</published><updated>2023-06-06T00:00:00-04:00</updated><id>https://ziyueqiu.github.io/blog/2023/JEDI%20(IMC%2022)</id><content type="html" xml:base="https://ziyueqiu.github.io/blog/2023/JEDI-(IMC-22)/"><![CDATA[<p><a href="https://groups.cs.umass.edu/ramesh/wp-content/uploads/sites/3/2022/11/JEDI.pdf">JEDI: Model-driven Trace Generation for Cache Simulations</a></p> <h2 id="what-you-should-know">What you should know</h2> <p>视角是：为想要开源 trace 又害怕泄露 PII (personal identifiable information) 的“公司”视角做 trace (re-)generation。</p> <p>model 叫 Popularity-Size Footprint Descriptor (pFD)，同时做到 request hit ratio (RHR) &amp; byte hit ratio (BHR) 准确，需要 LRU 算法，但是 eval 里面看到 non-LRU 也还不错。</p> <p>有趣的是，本文 claim 需要用绝对数值的 Popularity 数字而非 normalized 之后的某个百分比来产生 synthetic trace，因为</p> <p>Popularity distribution (POP) We define popularity of an object as <strong>the number of requests</strong> made for it in a given trace. Specifically, the popularity distribution POP(p) gives us the probability that an object is requested p times in the duration of the trace. Observe that popularity of an object depends on the length of the trace that is collected. If the length is doubled, the popularity of some objects in the trace could increase. However, we chose not to normalize the popularity by the trace length for the following reason. For sufficiently large traces, we find that there exist very few objects that are requested through the span of the trace. Most objects have a short lifespan i.e., the first and last request for an object are not far apart. Thus, if the trace is sufficiently large, POP remains unchanged as its length is increased.</p> <p>结合本文的 goal 是重新生成一个接近的 synthetic trace，所以目前的版本不支持生成更长的 trace。</p> <p>算法如下 Synthetic trace generator:</p> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/fig/JEDI-fig1-480.webp"/> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/fig/JEDI-fig1-800.webp"/> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/fig/JEDI-fig1-1400.webp"/> <img src="/assets/img/fig/JEDI-fig1.png" class="img-fluid rounded z-depth-1" width="auto" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <p>// 其实还有一部分是如何 mix traces 如果不止一个，这个我暂时用不上就忽略了。</p> <p>一些 eval，比如 Figure 10 &amp; 11 展示了各种算法效果都还不错（注意：JSynth 是本文的结果，其他是 baseline）</p> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/fig/JEDI-fig2-480.webp"/> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/fig/JEDI-fig2-800.webp"/> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/fig/JEDI-fig2-1400.webp"/> <img src="/assets/img/fig/JEDI-fig2.png" class="img-fluid rounded z-depth-1" width="auto" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure>]]></content><author><name></name></author><category term="paperreading"/><category term="modeling"/><category term="cache"/><summary type="html"><![CDATA[JEDI: Model-driven Trace Generation for Cache Simulations]]></summary></entry><entry><title type="html">Pesto (SOCC 11) 论文阅读</title><link href="https://ziyueqiu.github.io/blog/2023/Pesto-(SOCC-11)/" rel="alternate" type="text/html" title="Pesto (SOCC 11) 论文阅读"/><published>2023-04-14T00:00:00-04:00</published><updated>2023-04-14T00:00:00-04:00</updated><id>https://ziyueqiu.github.io/blog/2023/Pesto%20(SOCC%2011)</id><content type="html" xml:base="https://ziyueqiu.github.io/blog/2023/Pesto-(SOCC-11)/"><![CDATA[<p><a href="https://www.waldspurger.org/carl/papers/pesto-socc11.pdf">Pesto: Online Storage Performance Management in Virtualized Datacenters</a></p> <p>（没有搜到其他的中文博客，尽量写写）</p> <h2 id="abstract">Abstract</h2> <p>Previous: resort to gross overprovisioning or static partitioning of storage</p> <p>Pesto:</p> <ul> <li>completely automate storage performance management for virtualized datacenters</li> <li>provide IO load balancing with cost-benefit analysis, per-device congestion management, and initial placement of new workloads</li> </ul> <h2 id="introduction">Introduction</h2> <p>2 types of existing approaches:</p> <ul> <li>offline measurements</li> <li>analytical approaches relying on detailed device information</li> </ul> <h2 id="background">Background</h2> <p>Our goal is to monitor the array from the outside and predict performance metrics (e.g., expected average latency, peak device IOPS, proximity to peak performance) by utilizing a <strong>black-box approach</strong>. 这个 black-box approach 可以用最本质的方式解释，也就是 queueing theory，我在后面的位置解释。</p> <p>Define <code class="language-plaintext highlighter-rouge">Outstanding IOs</code>: When an application has requested for certain IO to be performed (reads or writes), we’ve already seen how its broken down into IO commands.These commands are sent to storage devices, and until they return and complete, they are termed as outstanding IO commands. –&gt; 其实就是 queueing</p> <h2 id="pesto-system-overview">Pesto System Overview</h2> <h2 id="lq-slope-performance-model">LQ-slope Performance Model</h2> <p>简单来说，这在 queueing theory 里，就是一个 closed-system benchmark（因此也不需要考虑 workload patterns，相比 open system），高 MPL (multi-player level) -&gt; “Unlike previous techniques, we do not try to model service times or IOPS for light workloads”（因此只需要知道 bottleneck 的数字，也就是 LQ-slope 代表的）。</p> <p>latency 和 queue length 之间满足 Little’s Law，throughput 在高 MPL 的情况下被 bottleneck 限制，可以直接算。</p> <p>后面略。</p> <h2 id="评论">评论</h2> <p>不能说不漂亮，虽然处理的是最简单的一类假设情况下的问题，但是我略掉的一部分也处理了很多根据预测出来的 latency 和 throughput 如何做 placement / load balancing / datastore removal or addition / cost analysis。</p>]]></content><author><name></name></author><category term="paperreading"/><category term="modeling"/><summary type="html"><![CDATA[Pesto: Online Storage Performance Management in Virtualized Datacenters]]></summary></entry><entry><title type="html">InftyDedup (FAST 23) 论文阅读</title><link href="https://ziyueqiu.github.io/blog/2023/InftyDedup-(FAST-23)/" rel="alternate" type="text/html" title="InftyDedup (FAST 23) 论文阅读"/><published>2023-03-01T00:00:00-05:00</published><updated>2023-03-01T00:00:00-05:00</updated><id>https://ziyueqiu.github.io/blog/2023/InftyDedup%20(FAST%2023)</id><content type="html" xml:base="https://ziyueqiu.github.io/blog/2023/InftyDedup-(FAST-23)/"><![CDATA[<p><a href="https://www.usenix.org/conference/fast23/presentation/kotlarska">InftyDedup: Scalable and Cost-Effective Cloud Tiering with Deduplication</a></p> <p>这方面了解不多，所以会记录一些知识：）</p> <h2 id="abstract">Abstract</h2> <ul> <li>maximize scalability by utilizing cloud services not only for storage but also for computation</li> <li>save cost by selecting between hot and cold cloud storage based on the characteristics of each data chunk</li> </ul> <h2 id="introduction">Introduction</h2> <ul> <li>Deduplication between different local tier systems is not supported for data stored in the cloud.</li> <li>Deduplication typically entails chunking data collections into smaller pieces that may be referenced multiple times, thereby possibly having different access patterns. This calls for finer-grained and more automated approaches to storage type selection.</li> </ul> <p>Conclusion:</p> <ul> <li>Like the existing tiering-to-cloud backup solutions, InftyDedup moves selected data from a local-tier system to the cloud, based on customer-specific backup policies.</li> <li>Rather than relying on deduplication methods of on-premise solutions, InftyDedup deduplicates data using the cloud infrastructure. (也就是说连 dedup 计算本身都可以做到不依赖本身的算力，读到现在我猜是也可以租 VM。) This is done periodically in batches before actually transferring data to the cloud, which, among others, enables the dynamic allocation of cloud resources.</li> </ul> <h2 id="background">Background</h2> <h3 id="deduplication-storage">Deduplication Storage</h3> <p>Firstly, the data stream is chunked into small immutable blocks of size from 2 KB to 128 KB. Secondly, each block receives a fingerprint, for instance, by computing the SHA-256 hash of the block’s data. Finally, the fingerprint is compared with other fingerprints in the system, and if the fingerprint is unique, the block’s data is written.</p> <p>The deduplicated blocks are typically organized in a directed acyclic graph. Each file has its root block corresponding to a vertex that references other blocks.</p> <h3 id="lifecycle-of-backups">Lifecycle of Backups</h3> <p>两个约束：</p> <ul> <li>the data should be available quickly in case of a disaster <ul> <li>e.g. Recovery Point Objectives of seconds and Recovery Time Objectives of minutes</li> </ul> </li> <li>older versions of backups need to be stored for weeks, months, or even years <ul> <li>backups are often moved to cheaper storage after a specific time</li> </ul> </li> </ul> <h3 id="cloud-storage">Cloud Storage</h3> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/fig/InftyDedup-fig1-480.webp"/> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/fig/InftyDedup-fig1-800.webp"/> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/fig/InftyDedup-fig1-1400.webp"/> <img src="/assets/img/fig/InftyDedup-fig1.png" class="img-fluid rounded z-depth-1" width="auto" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <h2 id="inftydedup-architecture">InftyDedup Architecture</h2> <p>The cloud tier stores deduplicated data with necessary persistent metadata, and occasionally executes highly optimized batch algorithms.</p> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/fig/InftyDedup-fig2-480.webp"/> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/fig/InftyDedup-fig2-800.webp"/> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/fig/InftyDedup-fig2-1400.webp"/> <img src="/assets/img/fig/InftyDedup-fig2.png" class="img-fluid rounded z-depth-1" width="auto" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <h3 id="cloud-cost-considerations">Cloud Cost Considerations</h3> <p>暂略</p> <h2 id="评论">评论</h2> <ul> <li>我们能认为用户级别的 dedup 是在榨取 cloud providers 的利润吗？因为他们肯定也会做 dedup。</li> <li>总的来说就是几方面的 tradeoff：deleted blocks 要存多久再做 GC？removal cost vs storage cost，根据 backup 身上的 expiration date 计算；要放在多便宜的 layer，根据各种 cost，比如 data movement / restore frequency 等等计算。对于 backup，这一切格外地好计算。</li> </ul>]]></content><author><name></name></author><category term="paperreading"/><category term="cloud"/><category term="deduplication"/><summary type="html"><![CDATA[InftyDedup: Scalable and Cost-Effective Cloud Tiering with Deduplication]]></summary></entry><entry><title type="html">GL-Cache (FAST 23) 论文阅读</title><link href="https://ziyueqiu.github.io/blog/2023/GL-Cache-(FAST-23)/" rel="alternate" type="text/html" title="GL-Cache (FAST 23) 论文阅读"/><published>2023-02-26T00:00:00-05:00</published><updated>2023-02-26T00:00:00-05:00</updated><id>https://ziyueqiu.github.io/blog/2023/GL-Cache%20(FAST%2023)</id><content type="html" xml:base="https://ziyueqiu.github.io/blog/2023/GL-Cache-(FAST-23)/"><![CDATA[<p><a href="https://www.usenix.org/system/files/fast23-yang.pdf">GL-Cache: Group-level learning for efficient and high-performance caching</a></p> <h2 id="introduction">Introduction</h2> <p>“learned caches”: employed machine learning to improve cache evictions</p> <ul> <li>object-level learning (e.g. LRB): learns the next access time for each object using dozens of object features and evicts the object with the furthest predicted request time <ul> <li>significant computation and storage overheads</li> <li>Zipf distributions “most objects only get a limited number of requests. This leads to limited object-level information for learning”</li> </ul> </li> <li>learning-from-distribution (e.g. LHD): models request probability distributions to inform eviction decisions <ul> <li>必须 random sample，利用的信息有限，绝大部分 object 在 distribution 的尾部、信息很少</li> </ul> </li> <li>learning-from-simple-experts (e.g. LeCaR and Cacheus): performs evictions by choosing eviction candidates recommended by experts (e.g., LRU and LFU), and updates experts’ weights based on their past performance on the workload <ul> <li>A delay exists between a bad eviction and an update on the expert’s weight (“delayed rewards” in reinforcement learning).</li> </ul> </li> </ul> <p>Group-level Learned Cache:</p> <ul> <li>cluster similar objects into groups using write time</li> <li>evict the least useful groups using a merge-based eviction</li> <li>introduce a group utility function to rank groups</li> </ul> <h2 id="background-and-motivation">Background and motivation</h2> <p>Questions:</p> <ul> <li>“Object-level learning needs to sample objects and perform inference at each write (eviction). For example, LRB samples 32 objects and copies their features to a matrix for inference for each eviction. In our measurement, each evic- tion (including feature copy, inference, and ranking) takes <strong>200 μs</strong> on one CPU core, indicating that the cache can evict at most 5,000 objects on a single core per second.” 200 us 这实验是怎么做的，这数字怎么这么大</li> </ul> <h2 id="gl-cache-group-level-learned-cache">GL-Cache: Group-level learned cache</h2> <ul> <li>Grouping by write time <ul> <li>PS: allow an efficient implementation using a log-structured cache</li> </ul> </li> </ul> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/fig/GLCache-fig1-480.webp"/> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/fig/GLCache-fig1-800.webp"/> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/fig/GLCache-fig1-1400.webp"/> <img src="/assets/img/fig/GLCache-fig1.png" class="img-fluid rounded z-depth-1" width="auto" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <ul> <li>Learning object-group utility <ul> <li>gradient boosting machines (GBM) because tree models do not require feature normalization</li> <li>formulate the learning task as a regression problem that minimizes the mean square loss (L2) of object-group utilities</li> <li>A sampled group may be evicted before being used for training. Such evictions halt the tracking of group utility. GL-Cache keeps ghost entries for objects which have not been factored into group utility.</li> <li>一次扔掉最差的 group 附近的 \(N_{group}\) 个 group，这个参数可能设置成整体的 1% 之类的</li> </ul> </li> </ul> <p>Questions:</p> <ul> <li>如果有好几个 application 同时写入，凭什么说有相似的 write-time 的 object 相似呢？</li> <li>“GL-Cache generates new training data by sampling cached object groups, and it copies the features of the sampled groups into a pre-allocated memory region” 这不是也有 random sampling (v.s. learning-from-distribution)?</li> </ul> <h2 id="evaluation">Evaluation</h2> <p>放两个有意思的图：</p> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/fig/GLCache-fig2-480.webp"/> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/fig/GLCache-fig2-800.webp"/> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/fig/GLCache-fig2-1400.webp"/> <img src="/assets/img/fig/GLCache-fig2.png" class="img-fluid rounded z-depth-1" width="auto" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/fig/GLCache-fig3-480.webp"/> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/fig/GLCache-fig3-800.webp"/> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/fig/GLCache-fig3-1400.webp"/> <img src="/assets/img/fig/GLCache-fig3.png" class="img-fluid rounded z-depth-1" width="auto" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure>]]></content><author><name></name></author><category term="paperreading"/><category term="Cache"/><category term="ML"/><summary type="html"><![CDATA[GL-Cache: Group-level learning for efficient and high-performance caching]]></summary></entry><entry><title type="html">Chandy-Lamport (TOCS 85) 论文阅读</title><link href="https://ziyueqiu.github.io/blog/2023/Chandy-Lamport-(TOCS-85)/" rel="alternate" type="text/html" title="Chandy-Lamport (TOCS 85) 论文阅读"/><published>2023-01-27T00:00:00-05:00</published><updated>2023-01-27T00:00:00-05:00</updated><id>https://ziyueqiu.github.io/blog/2023/Chandy-Lamport%20(TOCS%2085)</id><content type="html" xml:base="https://ziyueqiu.github.io/blog/2023/Chandy-Lamport-(TOCS-85)/"><![CDATA[<p><a href="https://lamport.azurewebsites.net/pubs/chandy.pdf">Distributed Snapshots: Determining Global States of Distributed Systems</a></p> <h2 id="评论">评论</h2> <p><a href="https://zhuanlan.zhihu.com/p/53482103">这篇文章</a> 比较简洁地过了一遍。</p>]]></content><author><name></name></author><category term="paperreading"/><category term="distributed system"/><summary type="html"><![CDATA[Distributed Snapshots: Determining Global States of Distributed Systems]]></summary></entry><entry><title type="html">Time, Clocks, and Ordering (78) 论文阅读</title><link href="https://ziyueqiu.github.io/blog/2023/Time,-Clocks,-and-Ordering-(78)/" rel="alternate" type="text/html" title="Time, Clocks, and Ordering (78) 论文阅读"/><published>2023-01-24T00:00:00-05:00</published><updated>2023-01-24T00:00:00-05:00</updated><id>https://ziyueqiu.github.io/blog/2023/Time,%20Clocks,%20and%20Ordering%20(78)</id><content type="html" xml:base="https://ziyueqiu.github.io/blog/2023/Time,-Clocks,-and-Ordering-(78)/"><![CDATA[<p><a href="https://dl.acm.org/doi/10.1145/359545.359563">Time, clocks, and the ordering of events in a distributed system</a></p> <h2 id="评论">评论</h2> <p>不愧是 Leslie Lamport，直接放<a href="http://zhangtielei.com/posts/blog-time-clock-ordering.html">这篇解析</a>，总结得真好啊</p>]]></content><author><name></name></author><category term="paperreading"/><category term="distributed system"/><summary type="html"><![CDATA[Time, clocks, and the ordering of events in a distributed system]]></summary></entry><entry><title type="html">XRP (OSDI 22) 论文阅读</title><link href="https://ziyueqiu.github.io/blog/2023/XRP-(OSDI-22)/" rel="alternate" type="text/html" title="XRP (OSDI 22) 论文阅读"/><published>2023-01-22T00:00:00-05:00</published><updated>2023-01-22T00:00:00-05:00</updated><id>https://ziyueqiu.github.io/blog/2023/XRP%20(OSDI%2022)</id><content type="html" xml:base="https://ziyueqiu.github.io/blog/2023/XRP-(OSDI-22)/"><![CDATA[<p><a href="https://www.usenix.org/conference/osdi22/presentation/zhong">XRP: In-Kernel Storage Functions with eBPF</a></p> <h2 id="abstract">Abstract</h2> <p>We present XRP, a framework that allows applications to execute user-defined storage functions, such as index lookups or aggregations, from an eBPF hook in the NVMe driver, safely bypassing most of the kernel’s storage stack.</p> <h2 id="introduction">Introduction</h2> <p>Complete kernel bypass (SPDK): force applications to imple- ment their own file systems, to forgo isolation and safety, and to poll for I/O completion which wastes CPU cycles when I/O utilization is low; suffer from high average and tail latencies and severely reduced throughput when the schedulable thread count exceeds the number of available cores.</p> <p>BPF is an OS-supported mechanism that ensures isolation, does not lead to low utilization due to busy-waiting, and allows a large number of threads or processes to share the same core, leading to better overall utilization.</p> <p>In order to maximize its performance benefit, XRP uses a hook in the NVMe driver’s interrupt handler, thereby bypassing the kernel’s block, file system and system call layers. This allows XRP to trigger BPF functions directly from the NVMe driver as each I/O completes, enabling quick resubmission of I/Os that traverse other blocks on the storage device.</p> <h2 id="background-and-motivation">Background and Motivation</h2> <p>Software is now the storage bottleneck:</p> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/fig/XRP-fig1-480.webp"/> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/fig/XRP-fig1-800.webp"/> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/fig/XRP-fig1-1400.webp"/> <img src="/assets/img/fig/XRP-fig1.png" class="img-fluid rounded z-depth-1" width="auto" height="auto" data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <h2 id="design-callenges-and-principles">Design Callenges and Principles</h2> <ul> <li>Challenge 1: address translation and security. <ul> <li>the BPF function could access any block on the device, including blocks that belong to a file that the user does not have permissions to access.</li> </ul> </li> <li>Challenge 2: concurrency and caching. <ul> <li>A write issued from the file system will only be reflected in the page cache, which is not visible to XRP. In addition, any writes that modify the layout of the data structure (e.g., modify the pointers to the next block) that are issued concurrently to read requests could lead XRP to accidentally fetch the wrong data. Both of these could be addressed by locking, but accessing locks from within the NVMe interrupt handler may be expensive.</li> </ul> </li> <li>Observation: most on-disk data structures are stable, e.g. LSM trees.</li> <li>Design principles. <ul> <li>One file at a time: only issue chained resubmission on a single file.</li> <li>Stable data structures: whose layout (i.e. pointers) remain immutable for a long period of time (i.e. seconds or more); do not plan to support operations that require locks during the traversal or iteration of data structures.</li> <li>User-managed caches: 不能有 os page cache</li> <li>Slow path fallback (best-effort)</li> </ul> </li> </ul> <h2 id="xrp-design-and-implementation">XRP Design and Implementation</h2> <p>暂略</p> <h2 id="case-studies">Case Studies</h2> <p>BPF-KV 这个比较有意思，如下：</p> <ul> <li>BPF-KV is designed to store a large number of small objects and to provide good read performance even under uniform access patterns. BPF-KV uses a B+-tree index to find the location of objects, and the objects themselves are stored in an unsorted log. For simplicity, BPF-KV uses fixed-sized keys (8 B) and values (64 B).</li> <li>Consider the case where BPF-KV is used to store 10 billion 64 B objects. In BPF-KV’s index, each node is 512 B (matching the access granularity of the Optane SSD); hence, the tree has a fanout of 31 (i.e. each internal node can store pointers to 31 children). Therefore, 10 billion objects would require an index with 8 levels. Fitting 6 index levels in DRAM is expensive and would require 14 GB, while fitting 7 levels or more becomes prohibitively expensive (437 GB of DRAM or more). So, to support a large number of keys, BPF-KV would require at the minimum 3-4 I/Os from storage for each lookup, including a final I/O to fetch the actual key-value pair from disk.</li> <li>Also note that having a hard memory budget for caching the index is common in many real-world key-value stores, since the index cache often competes with other parts of the system that need memory, such as filters and the object cache.</li> </ul> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/fig/XRP-fig2-480.webp"/> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/fig/XRP-fig2-800.webp"/> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/fig/XRP-fig2-1400.webp"/> <img src="/assets/img/fig/XRP-fig2.png" class="img-fluid rounded z-depth-1" width="auto" height="auto" data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <h2 id="评价">评价</h2> <p>use case 到底够不够广泛我不确定；software overhead 能看见的、算足够大的场景可能也就 Optane P5800X 了；assumption 有点多，比如 target static data structures，感觉有点困难。</p>]]></content><author><name></name></author><category term="paperreading"/><category term="storage"/><summary type="html"><![CDATA[XRP: In-Kernel Storage Functions with eBPF]]></summary></entry></feed>